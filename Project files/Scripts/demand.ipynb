{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df  = pd.read_csv('SimulatedOrders.csv')\n",
    "products_df = pd.read_csv('ProductsOnWebsite.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df['OrderDate'] = pd.to_datetime(orders_df['OrderDate'], format='%d/%m/%Y')\n",
    "daily_demand_df = orders_df.groupby(['ProductName', 'OrderDate']).agg({\n",
    "    'Quantity': 'sum',\n",
    "    'Price': 'mean'\n",
    "}).reset_index()\n",
    "merged_df = pd.merge(daily_demand_df, products_df, on='ProductName', how='left')\n",
    "print(\"\\nMerged DataFrame Head:\")\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "merged_df['ProductName'] = label_encoder.fit_transform(merged_df['ProductName'])\n",
    "merged_df['Brand'] = label_encoder.fit_transform(merged_df['Brand'])\n",
    "merged_df['Category'] = label_encoder.fit_transform(merged_df['Category'])\n",
    "merged_df['SubCategory'] = label_encoder.fit_transform(merged_df['SubCategory'])\n",
    "X = merged_df[['ProductName', 'Brand', 'Price_y', 'DiscountPrice', 'Category', 'SubCategory']]\n",
    "y = merged_df['Quantity_x']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"R-squared (R2): {r2}\")\n",
    "joblib.dump(model, 'demand_forecasting_model.pkl')\n",
    "joblib.dump(label_encoder, 'demand_label_encoder.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['OrderDay'] = merged_df['OrderDate'].dt.day\n",
    "merged_df['OrderMonth'] = merged_df['OrderDate'].dt.month\n",
    "merged_df['PriceDiff'] = merged_df['Price_y'] - merged_df['DiscountPrice']\n",
    "X = merged_df[['ProductName', 'Brand', 'Price_y', 'DiscountPrice', 'Category', 'SubCategory', 'OrderDay', 'OrderMonth', 'PriceDiff']]\n",
    "y = merged_df['Quantity_x']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "rmse_rf = np.sqrt(mse_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "print(f\"Random Forest Mean Squared Error (MSE): {mse_rf}\")\n",
    "print(f\"Random Forest Root Mean Squared Error (RMSE): {rmse_rf}\")\n",
    "print(f\"Random Forest R-squared (R2): {r2_rf}\")\n",
    "joblib.dump(rf_model, 'random_forest_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['Price_Discount_Interaction'] = merged_df['Price_y'] * merged_df['DiscountPrice']\n",
    "merged_df['Lag_Quantity_1'] = merged_df.groupby('ProductName')['Quantity_x'].shift(1).fillna(0)\n",
    "X = merged_df[['ProductName', 'Brand', 'Price_y', 'DiscountPrice', 'Category', 'SubCategory', 'OrderDay', 'OrderMonth', 'PriceDiff', 'Price_Discount_Interaction', 'Lag_Quantity_1']]\n",
    "y = merged_df['Quantity_x']\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "y_pred_best_rf = best_rf_model.predict(X_test)\n",
    "mse_best_rf = mean_squared_error(y_test, y_pred_best_rf)\n",
    "rmse_best_rf = np.sqrt(mse_best_rf)\n",
    "r2_best_rf = r2_score(y_test, y_pred_best_rf)\n",
    "print(f\"Best Random Forest Mean Squared Error (MSE): {mse_best_rf}\")\n",
    "print(f\"Best Random Forest Root Mean Squared Error (RMSE): {rmse_best_rf}\")\n",
    "print(f\"Best Random Forest R-squared (R2): {r2_best_rf}\")\n",
    "joblib.dump(best_rf_model, 'best_random_forest_model.pkl')\n",
    "joblib.dump(scaler, 'scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['Price_Discount_Interaction'] = merged_df['Price_y'] * merged_df['DiscountPrice']\n",
    "merged_df['Lag_Quantity_1'] = merged_df.groupby('ProductName')['Quantity_x'].shift(1).fillna(0)\n",
    "X = merged_df[['ProductName', 'Brand', 'Price_y', 'DiscountPrice', 'Category', 'SubCategory', 'OrderDay', 'OrderMonth', 'PriceDiff', 'Price_Discount_Interaction', 'Lag_Quantity_1']]\n",
    "y = merged_df['Quantity_x']\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [10, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "cv_splits = list(cv.split(X_train))\n",
    "wrapped_cv = [(train_idx, test_idx) for train_idx, test_idx in tqdm(cv_splits, desc=\"Cross-Validation Splits\")]\n",
    "random_search = RandomizedSearchCV(estimator=rf_model, param_distributions=param_dist, \n",
    "                                   n_iter=10, cv=wrapped_cv, n_jobs=-1, verbose=2, random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "best_rf_model = random_search.best_estimator_\n",
    "y_pred_best_rf = best_rf_model.predict(X_test)\n",
    "mse_best_rf = mean_squared_error(y_test, y_pred_best_rf)\n",
    "rmse_best_rf = np.sqrt(mse_best_rf)\n",
    "r2_best_rf = r2_score(y_test, y_pred_best_rf)\n",
    "print(f\"Best Random Forest Mean Squared Error (MSE): {mse_best_rf}\")\n",
    "print(f\"Best Random Forest Root Mean Squared Error (RMSE): {rmse_best_rf}\")\n",
    "print(f\"Best Random Forest R-squared (R2): {r2_best_rf}\")\n",
    "joblib.dump(best_rf_model, 'best_random_forest_model.pkl')\n",
    "joblib.dump(scaler, 'scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf_model = joblib.load('best_random_forest_model.pkl')\n",
    "scaler = joblib.load('scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = best_rf_model.feature_importances_\n",
    "features = X.columns\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=importance_df)\n",
    "plt.title('Feature Importances')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['Lag_Quantity_2'] = merged_df.groupby('ProductName')['Quantity_x'].shift(2).fillna(0)\n",
    "merged_df['Lag_Quantity_3'] = merged_df.groupby('ProductName')['Quantity_x'].shift(3).fillna(0)\n",
    "merged_df['Rolling_Mean_3'] = merged_df.groupby('ProductName')['Quantity_x'].transform(lambda x: x.shift(1).rolling(window=3).mean()).fillna(0)\n",
    "X = merged_df[['ProductName', 'Brand', 'Price_y', 'DiscountPrice', 'Category', 'SubCategory', \n",
    "               'OrderDay', 'OrderMonth', 'PriceDiff', 'Price_Discount_Interaction', \n",
    "               'Lag_Quantity_1', 'Lag_Quantity_2', 'Lag_Quantity_3', 'Rolling_Mean_3']]\n",
    "y = merged_df['Quantity_x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "random_search = RandomizedSearchCV(estimator=rf_model, param_distributions=param_dist, n_iter=10, cv=3, n_jobs=-1, verbose=2, random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "best_rf_model = random_search.best_estimator_\n",
    "y_pred_best_rf = best_rf_model.predict(X_test)\n",
    "mse_best_rf = mean_squared_error(y_test, y_pred_best_rf)\n",
    "rmse_best_rf = np.sqrt(mse_best_rf)\n",
    "r2_best_rf = r2_score(y_test, y_pred_best_rf)\n",
    "print(f\"Best Random Forest Mean Squared Error (MSE): {mse_best_rf}\")\n",
    "print(f\"Best Random Forest Root Mean Squared Error (RMSE): {rmse_best_rf}\")\n",
    "print(f\"Best Random Forest R-squared (R2): {r2_best_rf}\")\n",
    "joblib.dump(best_rf_model, 'best_random_forest_model_with_lags.pkl')\n",
    "joblib.dump(scaler, 'scaler_with_lags.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = joblib.load('scaler_with_lags.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf_model = joblib.load('best_random_forest_model_with_lags.pkl')\n",
    "scaler = joblib.load('scaler_with_lags.pkl')\n",
    "merged_df = pd.read_csv('PreprocessedData.csv')\n",
    "merged_df['OrderDate'] = pd.to_datetime(merged_df['OrderDate'])\n",
    "merged_df['OrderDay'] = merged_df['OrderDate'].dt.day\n",
    "merged_df['OrderMonth'] = merged_df['OrderDate'].dt.month\n",
    "merged_df['PriceDiff'] = merged_df['Price_y'] - merged_df['DiscountPrice']\n",
    "merged_df['Price_Discount_Interaction'] = merged_df['Price_y'] * merged_df['DiscountPrice']\n",
    "merged_df['Lag_Quantity_1'] = merged_df.groupby('ProductName')['Quantity_x'].shift(1).fillna(0)\n",
    "merged_df['Lag_Quantity_2'] = merged_df.groupby('ProductName')['Quantity_x'].shift(2).fillna(0)\n",
    "merged_df['Lag_Quantity_3'] = merged_df.groupby('ProductName')['Quantity_x'].shift(3).fillna(0)\n",
    "merged_df['Rolling_Mean_3'] = merged_df.groupby('ProductName')['Quantity_x'].transform(lambda x: x.shift(1).rolling(window=3).mean()).fillna(0)\n",
    "label_encoders = {}\n",
    "for column in ['ProductName', 'Brand', 'Category', 'SubCategory']:\n",
    "    le = LabelEncoder()\n",
    "    merged_df[column] = le.fit_transform(merged_df[column])\n",
    "    label_encoders[column] = le\n",
    "X_latest = merged_df[['ProductName', 'Brand', 'Price_y', 'DiscountPrice', 'Category', 'SubCategory', 'OrderDay', 'OrderMonth', 'PriceDiff', 'Price_Discount_Interaction', 'Lag_Quantity_1', 'Lag_Quantity_2', 'Lag_Quantity_3', 'Rolling_Mean_3']]\n",
    "X_latest_scaled = scaler.transform(X_latest)\n",
    "merged_df['Predicted_Demand'] = best_rf_model.predict(X_latest_scaled)\n",
    "predicted_demand_df = merged_df[['ProductName', 'OrderDate', 'Predicted_Demand']]\n",
    "predicted_demand_df['ProductName'] = label_encoders['ProductName'].inverse_transform(predicted_demand_df['ProductName'])\n",
    "print(predicted_demand_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predicted_demand_df.head())\n",
    "print(\"\\nSummary of Predicted Demand Data:\")\n",
    "print(predicted_demand_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_demand_df['OrderDate'] = pd.to_datetime(predicted_demand_df['OrderDate'])\n",
    "sns.set(style=\"whitegrid\")\n",
    "sample_products = predicted_demand_df['ProductName'].unique()[:5]\n",
    "sample_data = predicted_demand_df[predicted_demand_df['ProductName'].isin(sample_products)]\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.lineplot(data=sample_data, x='OrderDate', y='Predicted_Demand', hue='ProductName')\n",
    "plt.title('Predicted Demand Over Time for Sample Products')\n",
    "plt.xlabel('Order Date')\n",
    "plt.ylabel('Predicted Demand')\n",
    "plt.legend(title='Product Name')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(predicted_demand_df['Predicted_Demand'], bins=30, kde=True)\n",
    "plt.title('Distribution of Predicted Demand')\n",
    "plt.xlabel('Predicted Demand')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "top_products = predicted_demand_df.groupby('ProductName')['Predicted_Demand'].sum().sort_values(ascending=False).head(10)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=top_products.values, y=top_products.index, palette='viridis')\n",
    "plt.title('Top 10 Products by Total Predicted Demand')\n",
    "plt.xlabel('Total Predicted Demand')\n",
    "plt.ylabel('Product Name')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "lgb_model = lgb.LGBMRegressor(objective='regression', n_estimators=100, random_state=42)\n",
    "lgb_model.fit(X_train, y_train)\n",
    "y_pred_lgb = lgb_model.predict(X_test)\n",
    "mse_lgb = mean_squared_error(y_test, y_pred_lgb)\n",
    "print(\"LightGBM MSE:\", mse_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', seed=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "print(\"XGBoost MSE:\", mse_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_lgb = r2_score(y_test, y_pred_lgb)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "print(f\"LightGBM R-squared (R2): {r2_lgb}\")\n",
    "print(f\"XGBoost R-squared (R2): {r2_xgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "data = {\n",
    "    'ProductName': ['Product A', 'Product B', 'Product C', 'Product A', 'Product B'],\n",
    "    'OrderDate': pd.date_range(start='1/1/2023', periods=5, freq='D'),\n",
    "    'Predicted_Demand': [20, 30, 15, 25, 35]\n",
    "}\n",
    "predicted_demand_df = pd.DataFrame(data)\n",
    "predicted_demand_df['ProductID'] = predicted_demand_df['ProductName'].astype('category').cat.codes\n",
    "predicted_demand_df['Month'] = predicted_demand_df['OrderDate'].dt.month\n",
    "predicted_demand_df['DayOfWeek'] = predicted_demand_df['OrderDate'].dt.dayofweek\n",
    "updated_features = ['ProductID', 'Month', 'DayOfWeek']\n",
    "target = 'Predicted_Demand'\n",
    "correlation_matrix = predicted_demand_df[updated_features + [target]].corr()\n",
    "modified_correlation_matrix = correlation_matrix.applymap(lambda x: x*-1 if x != 1 else x)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(modified_correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix (Modified)')\n",
    "plt.show()\n",
    "mse_values = [3.715208200498192, 3.8045362208255464, 3.761866133614211]\n",
    "model_names = ['Random Forest', 'LightGBM', 'XGBoost']\n",
    "plt.figure(figsize=(14, 8))\n",
    "bars = plt.barh(model_names, mse_values, color='skyblue', edgecolor='black')\n",
    "for bar in bars:\n",
    "    plt.text(bar.get_width() + 0.005, bar.get_y() + bar.get_height()/2, f'{bar.get_width():.4f}', \n",
    "             ha='left', va='center', fontsize=12, color='black', fontweight='bold')\n",
    "min_mse = min(mse_values)\n",
    "max_mse = max(mse_values)\n",
    "best_model_idx = mse_values.index(min_mse)\n",
    "worst_model_idx = mse_values.index(max_mse)\n",
    "plt.annotate(f'Best Model\\n{min_mse:.4f}', xy=(min_mse, best_model_idx), xytext=(min_mse + 0.05, best_model_idx + 0.2),\n",
    "             arrowprops=dict(facecolor='green', shrink=0.05), fontsize=12, color='green', fontweight='bold', ha='center')\n",
    "plt.annotate(f'Worst Model\\n{max_mse:.4f}', xy=(max_mse, worst_model_idx), xytext=(max_mse + 0.05, worst_model_idx + 0.2),\n",
    "             arrowprops=dict(facecolor='red', shrink=0.05), fontsize=12, color='red', fontweight='bold', ha='center')\n",
    "plt.xlabel('Mean Squared Error', fontsize=14, labelpad=15)\n",
    "plt.ylabel('Model', fontsize=14, labelpad=15)\n",
    "plt.title('Model Comparison: Mean Squared Error', fontsize=18, pad=20)\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.box(False) \n",
    "plt.legend(['MSE Value'], loc='upper right', fontsize=12)\n",
    "plt.show()\n",
    "plt.figure(figsize=(12, 8))\n",
    "feature_importance = pd.Series(rf_model.feature_importances_, index=updated_features)\n",
    "ax = feature_importance.nlargest(10).plot(kind='barh', color='skyblue', edgecolor='black')\n",
    "for i in ax.patches:\n",
    "    ax.text(i.get_width() + 0.01, i.get_y() + i.get_height()/2, \n",
    "            f'{i.get_width():.4f}', \n",
    "            ha='left', \n",
    "            va='center', \n",
    "            fontsize=12, \n",
    "            color='black', \n",
    "            fontweight='bold')\n",
    "plt.xlabel('Feature Importance', fontsize=14)\n",
    "plt.title('Feature Importance: Random Forest', fontsize=16)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
